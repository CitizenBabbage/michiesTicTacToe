XXXXXXXXXXXXXXXXXXXXXXXXXX

1. Recode the net generator to take an input of 27. 
2. hidden layers should be size 32
3. output layers should be size 36
4. checks are going to go haywire. Remove nine check from tensor check. 
5. softmax outputs in groups of four
6. translate from softmax output to boardstate
7. Change the minimax numerize function to return binary arrays of four digits. 

Reorganize the network architecture as follows: 

1. Encode the input data as a vector of length 3 using one hot encoding. I.e. 
X = [1, 0, 0]
O = [0, 1, 0]
_ = [0, 0, 1]

2. That means the whole input board can be flattened to an array of 27. 

3. The task is to train up a classifier for each square, where there are four possible classifications per square, namely: win, draw, loss, impossible

4. We can train up the net to predict one square for the sake of time efficiency while settling on the architecture. Once we know that an architecture that predicts for one square works, it follows that it will work if we simply expand the output layer to make nine squares. 

5. We can encode the outputs using softmax as follows: 
win = [1,0,0,0]
draw = [0,1,0,0]
loss = [0,0,1,0]
impossible = [0,0,0,1]

6. So the architecture (to predict one square) will have 27 inputs and four outputs. To predict all nine, it will ultimately have 27 inputs and 36 outputs. 

XXXXXXXXXXXXXXXXXXXXXXXXXX

create a simple value setter for the minimax values. 

make sure what's coming out as an array is an array of nine elements, not an array containing an array of nine elements. 

XXXXXXXXXXXXXXXXXXXXXXXXXX

1. run one iteration. 
2. one iteration ends by changing the net. 
3. useEffect, d: net, sets new error value. 
4. useEffect, d: error value, if iteration > 0, drops the iteration count by 1. 
5. useEffect d: iteration count: if the iteration count is > 1, launch one iteration. 

XXXXXXXXXXXXXXXXXXXXXXXX

recover 2 layer neural net functionality DONE 
	shorten inputs nets to length 4 DONE
rewrite functions, if necessary, so that nets are passed as arrays DONE
rewrite neuroChooseMove to have subordinate function: prepareNet DONE
	check for functionality DONE
	rewrite prepareNet to deal with arbitrary net DONE BUT UNTESTED ON NETS != length 4
	check for functionality DONE BUT UNTESTED ON NETS != length 4
rewrite neuroChooseMove to pass arbitrary sized array for net DONE BUT UNTESTED ON NETS != length 4
	check for functionality DONE BUT UNTESTED ON NETS != length 4
rewrite forward pass function to iterate through n layers, should still work with 2.DONE BUT UNTESTED ON NETS != length 4
	check for functionality  DONE BUT UNTESTED ON NETS != length 4, UNTESTED FOR INTERACTION WITH LEARNING
	
CHANGE NET variable name!!

rewrite oneLearningIteration to learn through n layers. 
	correct receipt of forwardPass data DONE
	remove condition for checking sigma DONE
Rewrite all functions in cycle to pass OUTPUT nets as arrays
	That includes oneTrainingCycle DONE
	That includes oneLearningIteration DONE
	That includes runToMaxCycle DONE
remove the dummy values? DONE
Rewrite oneLearningIteration to iterate through layers DONE
	change runToMaxCycle to call it appropriately DONE
write a function that takes a number and creates a net with that many layers
test with 3
in oneLearningIteration, return condition for checking sigma 
	pass sqrdError as an argument, value infinity, from oneLearningCycle
	return immediately if sqrdError < sigma
	update sqrdError as we learn

look at minimaxChooseMove. it seems to be returning full for every square in every board state. 

XXXXXXXXXXXXXXXXXXXX

in computeErrorForLastLayer, if we have the conditionalized error function, it tends to always send back 0s for some reason. If we change it to the non-conditionalized error function, we get all kinds of other errors. 

XXXXXXXXXXXXXXXXXXXX

the problem with any key returning you to the pick opponents page is still present DONE

More work needs to be done on the minimax targets, to give the NN a good target to aim at. Specifically, what is the difference between "no guaranteed result" and "draw"? Can you get a no guaranteed result, or does MM always deterministically give you a win, draw or loss? Why is it giving us 1 point for takeable squares? I think the squares are reporting as 1 because they are filled in in the final analysis. DONE

get the choose function to output the hiddensums for the new layer

change error function so that anything within 0.49 of the right answer counts as error 0. 

refactor oneTrainingCycle so that it doesn't have results[4] etc. 

get a better measure of overall error DONE

Change the way the board is encoded so that empty squares have a non-zero value. DONE

get the calculated error to appear beneath the displayed test case. DONE

XXXXXXXXXXXXXXXXXXXX

Rewrite gettrainingSet so that it selects from the complete array of boardstates, not just the archetypal ones. DONE

Create a testing display that selects randomly from the TRAINING set and shows the prediction vs the minimax response. This is to ensure that the training set, at least, has been mastered to the degree advertised, and that the control is working accordingly. DONE
	call new component on neurotraining page
	new component calls two board states side by side
	it runs neurochoose move on one and minimax on the other and displays the results

write a button that lets you keep getting new comparisons DONE
write another button that lets you get comparisons from the total set DONE
XXXXXXXXXXXXXXXXXXXXX


rewrite neuroChooseMove so that it returns an array, first argument is the usual return but second is the board prediction. DONE
Then rewrite choosemove so that it takes that in.  DONE

XXXXXXXXXXXXXXXXXXXXX

I think the board being passed as priorlayer is in the form of nulls, X's and O's instead of numerical. 

XXXXXXXXXXXXXXXXXXXXX

0. Mend training page for menace. 
1. Put the training option in the choose side page for neuro
2. break the training page up into distnct training pages based on who is training. 


XXXXXXXXXXXXXXXXXXXXXX
I need to go fetch the calculated, pre-relued values for each layer and somehow make them available to the update function. 

1. Add a bias. 
	Go to thinking and add two bias useStates with dummy values
	Arrange so these are passed to the choice function inside the foeSpecs
DONE

2. Make the forwardPass function output an array with the layer sums also outputted.
DONE

3. Pass that to the neuro choice function.
DONE

4. Make the general choice function grab the first value from the array. 
DONE

5. write a function that takes a minimax array and changes 10 into 1, -10 into -1, 0 into 0 and null into -2. 
DONE

6. Get the neuro update function to call the neuro choice function and take the layer sums (i.e. elements 2 and 3) from the array returned by the neuro choice function
DONE

7. finish the error function by inputting the sum into the derivative. 
DONE

8a. Add the activation values of the presynaptic neurons to the outputted data from the choice function. 
DONE

8b. Finish the update function for the final layer. 
DONE

8c. Write the error function for the hidden layers. 
DONE

8d. Write the update function for the hidden layers. 
DONE

8e. Get the differentials array by applying the differential formula to every member of the sumArrays returned in the forwardPass data. 
DONE



9. wrap the error function in a function that (a) calls the choice function for an inputted board state and gets the returned values, (b) calls minimax on the same board state and uses its output to create the label array. 
10. write a function that takes a percentage input and calls that percent of the board states from the database at random, returning the new database. 
11. write a wrapper that takes a percentage input, calls function 9 to create a new db then, for each element in the db, runs the update function on it. 

XXXXXXXXXXXXXXXXXXXXXX


revise choice function so that the mathematical operations are distinct and called by the main function. 

encase the choice functions in a recorder of the form 

const grad = tf.grad(f);

Now I can compute the gradient as follows: 

const dydx = grad();





XXXXXXXXXXXXXXXXXXXXXX


boardstate
connections from boardstate to hidden layer
hiddenLayer
connections from hiddenlayer to outputNodes
outputNode

error function compares two arrays: 
	the array of nine produced by the neural net. Each element is a score for what moving to that square is (predicted to be) worth
	the array of nine produced by minimax, likewise. 

Training occurs by picking (legal) boardstates from the database at random and asking the net to produce an array. 

Q: What should occupied squares be scored as? 

// returns a number from 0 through 8 
function forwardPass(){}

function predictAndCalculateError(boardState){
	prediction = forwardPass(boardState)
	actual = minimaxChooseMove(...)
	return calculateError(prediction,actual)
}

function calculateError(prediction,actual){
	if prediction === actual return 0
	else if prediction has same minimax score as actual return 0 
	else if prediction is on same line as actual return 1
	else return 2
}